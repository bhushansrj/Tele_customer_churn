# -*- coding: utf-8 -*-
"""Tele_customer_churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KnX1nLSCoC5MG8nuvmGZvSkvphH7BgoJ
"""

# Commented out IPython magic to ensure Python compatibility.
import time
import pandas as pd
import numpy as np
import h5py
from PIL import Image
from scipy import ndimage
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams['figure.figsize']=(5.0,4.0)
plt.rcParams['image.interpolation']='nearest'
plt.rcParams['image.cmap']='gray'
# %load_ext autoreload
# %autoreload 2
np.random.seed(1)

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv("/content/drive/MyDrive/customer_churn_dataset")

"""# New section"""

df.head()

df.describe()

df.info()

df.isnull().sum()

import seaborn as sns

plt.figure(figsize=(10,6))
churn_value=df['Churn'].value_counts()
plt.pie(churn_value,labels=churn_value.index,autopct='%.0f%%')
plt.title('HEATMAP')
plt.show()

df.nunique()

df['TotalCharges']=pd.to_numeric(df['TotalCharges'],errors='coerce')

plt.figure(figsize=(20,8))
sns.displot(df.tenure, color='red')
plt.show()

df['tenure']=df['tenure'].replace(0,df['tenure'].mean())

df[df['tenure']==0]

df.drop('customerID',axis=1,inplace=True)

plt.figure(figsize=(20,15))
sns.heatmap(df,anoot=True,cmap='viridis')
plt.show()

df=df.dropna(subset=['TotalCharges'])

from sklearn.preprocessing import LabelEncoder
col=df.select_dtypes(include=object).columns
le=LabelEncoder()
clean_df=df.copy()
for i in col:
  clean_df[i]=le.fit_transform(clean_df[i])

corr_df=clean_df.corr().round(2)
plt.figure(figsize=(12,9))
sns.heatmap(corr_df,annot=True,cmap='coolwarm')
plt.show()

clean_df.drop(['TotalCharges'],axis=1,inplace=True)

clean_df.head()

clean_df.to_csv("claened_churn_dataset.csv")

data=pd.read_csv("claened_churn_dataset.csv")

x=data.drop(['Churn'],axis=1)
y=data['Churn']

x.shape

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.8)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression(max_iter=10000)
lr.fit(x_train,y_train)
y_pred=lr.predict(x_test)

report=classification_report(y_test,y_pred)
accuracy=accuracy_score(y_test,y_pred)
print(f"Accuracy:{accuracy}")
print(report)

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
model1=SVC(random_state=101)

model1.fit(x_train,y_train)
y_pred=model1.predict(x_test)
accuracy=accuracy_score(y_test,y_pred)
report=classification_report(y_pred,y_test)
print(f"Accuracy:{accuracy}")
print(report)

model2=DecisionTreeClassifier(random_state=101)

model2.fit(x_train,y_train)
y_pred=model2.predict(x_test)
accuracy=accuracy_score(y_test,y_pred)
report=classification_report(y_pred,y_test)
print(f"Accuracy:{accuracy}")
print(report)

model3=RandomForestClassifier(random_state=101)

model3.fit(x_train,y_train)
y_pred=model3.predict(x_test)
accuracy=accuracy_score(y_test,y_pred)
report=classification_report(y_pred,y_test)
print(f"Accuracy:{accuracy}")
print(report)

from sklearn.model_selection import GridSearchCV
params1={'min_samples_leaf':[2,4,5],
        'max_depth':[2,3,4],}
grid_search=GridSearchCV(model2,params1,cv=5)
grid_search.fit(x_train,y_train)
grid_search.best_estimator_

print(grid_search.best_score_)

params2={'min_samples_leaf':[2,4,5],
        'max_depth':[2,3,4],
         'max_features':[5,10,12]}
grid_search=GridSearchCV(model3,params2,cv=5)
grid_search.fit(x_train,y_train)
grid_search.best_estimator_

grid_search.best_score_

from sklearn.model_selection import GridSearchCV
params={'C':[0.01,0.1,1,10],
        'gamma':[0.01,.1,1],
        'kernel':["rbf"],
        }
grid_search=GridSearchCV(model1,params,cv=5)
grid_search.fit(x_train,y_train)
grid_search.best_estimator_

grid_search.best_score_